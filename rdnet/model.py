import torch
from torch import nn
from torch.nn import functional as F

from einops import rearrange, repeat
from einops.layers.torch import Rearrange
from blocks import InjectionBlock, ScratchBlock, ReassembleBlock, RefineBlock, Interpolate
from kornia import filters


class KnowledgeFusion(nn.Module):
    '''
    Description: Iteratively injects the prior knowledge from visual detector into image patches so that relational information
                 could be used by further modules in downstream tasks such as depth prediction
    Params:
        - emb_size: Dimension of visual embedding
        - dims: Dimensions of latent represetations for each of the InjectionBlock inside this module
        - max_patches: Maximum number of patches for each of the input image
        - patch_dim: Dimension of each input patch
        - patch_size: the one-sided dimension of initial patch used to compute the normalized locations of objects
    '''

    def __init__(self, emb_size, dims, max_patches, patch_dim, patch_size, **kwargs):
        super().__init__()
        self.patch_size = patch_size
        layers = [InjectionBlock(
            emb_size=emb_size, inp_dim=patch_dim, out_dim=dims[0], max_patches=max_patches, **kwargs)]

        for idx in range(len(dims) - 1):
            layers.append(InjectionBlock(
                emb_size=dims[idx], inp_dim=dims[idx], out_dim=dims[idx + 1], max_patches=max_patches, **kwargs))

        self.layers = nn.ModuleList(layers)

    def patching(self, locations, patch_size):
        locs = locations
        locs[:, :, :2] -= locs[:, :, :2] % patch_size
        locs[:, :, 2:] += (patch_size - locs[:, :, 2:] % patch_size)
        return locs

    def forward(self, patches, embs, locations):
        '''
        Params:
            - patches: Input image patches as flatten
            - embs: Embedding tensors of multiple objects for each of the input image
            - locations: Locations of objects inside each image associated with embs
        Return: A latent patches represent the information of each image after the fusion with prior knowledge
        '''
        embs = torch.cat([embs, embs.mean(1, True)], dim=1)
        b, n, _ = embs.shape

        locs = self.patching(locations, patch_size=self.patch_size)
        masks = torch.zeros(patches.shape[:3], dtype=torch.bool).to(embs.device)
        masks = repeat(masks, 'b h w -> b n h w', n=n)

        img_loc = torch.LongTensor([0, 0, patches.shape[1], patches.shape[2]]).to(locs.device)
        img_locs = repeat(img_loc, 'd -> b n d', b=b, n=1)
        locs = torch.cat([locs, img_locs], dim=1)

        for idx, loc in enumerate(locs):
            for jdx, obj_loc in enumerate(loc):
                masks[idx, jdx, obj_loc[0]:obj_loc[2],
                      obj_loc[1]:obj_loc[3]] = True

        masks = rearrange(masks, 'b n h w -> (b n) (h w)')
        patches = repeat(patches, 'b h w d -> b n (h w) d', n=n)

        for layer in self.layers:
            patches, embs = layer(patches, embs, masks)

        masks = repeat(masks, '(b n) p -> b n p c', n=n, c=1)
        result = (patches * masks).sum(dim=1) / masks.sum(dim=1)
        return result


class DensePrediction(nn.Module):
    '''
    Description: Performing dense regression on individual pixel of depth map, due to the dense nature of the problem,
                 the head layer of this module use Conv and UpConv layers instead of Linear layer to extrapolate
                 the prediction from low resolution to higher one as seen in the head property.
    Params:
        - inp_dim: Dimension of input patches which are generated by KnowledgeFusion module
        - hidden_dims: A list of 4 numbers specify the dimension of information to be extracted at each of the 4 hooks
        - out_dim: The number of channels in the fused image-like tensor predicted by RefineBlock,
                   used to modify the fineness of output depths
        - activation: torch.nn activation function to be used for any activation layer in this module
    '''
    def __init__(
        self,
        inp_dim,
        hidden_dims,
        out_dim,
        activation,
        **kwargs
    ):
        super().__init__()

        self.scratch = ScratchBlock(
            hidden_dim=inp_dim,
            **kwargs
        )

        self.reassemble = ReassembleBlock(
            inp_dim=inp_dim,
            out_dims=hidden_dims,
            **kwargs
        )

        self.refine = RefineBlock(
            in_shape=hidden_dims,
            out_shape=out_dim,
            activation=activation,
            **kwargs
        )

    def forward(self, embs):
        '''
        Params: Relational awared image patches from KnowledgeFusion module
        Return: Estimated depth map of the original image
        '''
        results = self.scratch(embs)
        results = self.reassemble(results)
        results = self.refine(results)

        return results


class RDNet(nn.Module):
    '''
    Description: The class encompasses the whole RDNet model in which depth estimation is generated by analyzing
                 relations between objects in an image and projecting derived information from latent space onto
                 2D depth map with locality preserved.
    Params:
        - image_size: Tuple of two maximal dimensions for the input image
        - patch_size: One-sided dimension for initial patches
        - knowledge_dims: A list of dimensions for latent representations of each InjectionBlock
        - dense_dims: A list of dimensions for extracted information from ReassambleBlock
        - latent_dim: Number of channels for the image-like tensor before the head layer of DensePrediction module
    '''

    def __init__(self, image_size, patch_size, knowledge_dims, dense_dims, latent_dim,
                 activation, scale=1.0, shift=0.0, invert=False, channels=3, **kwargs
                 ):
        super().__init__()
        self.scale = scale
        self.shift = shift
        self.invert = invert

        patch_dim = channels * patch_size ** 2
        num_patches = (image_size[0] / patch_size, image_size[1] / patch_size)
        max_patches = num_patches[0] * num_patches[1]

        self.to_patch = Rearrange('b c (h p1) (w p2) -> b h w (p1 p2 c)',
                                  p1=int(patch_size), p2=int(patch_size))
        self.knowledge = KnowledgeFusion(
            dims=knowledge_dims,
            max_patches=max_patches,
            patch_dim=patch_dim,
            patch_size=patch_size,
            **kwargs
        )
        self.dense = DensePrediction(
            inp_dim=knowledge_dims[-1],
            hidden_dims=dense_dims,
            out_dim=latent_dim,
            max_patches=max_patches,
            num_patches=num_patches,
            activation=activation,
            **kwargs
        )
        self.head = nn.Sequential(
            nn.Conv2d(latent_dim, latent_dim // 2,
                      kernel_size=3, stride=1, padding=1),
            Interpolate(scale_factor=2, mode="bilinear"),
            nn.Conv2d(latent_dim // 2, 32, kernel_size=3, stride=1, padding=1),
            activation(True),
            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),
            activation(True)
        )

    def forward(self, images, embs, locations):
        '''
        Params:
            - images: A batch of multiple input images
            - embs: A batch of multiple embedding matrices as genereted by visual detector
            - locations: A batch of multiple locations associated with each of the embedding in embs
        Return: Final depth estimation for each images in the input batch
        '''
        assert (images * images).sum() > 1e-6
        patches = self.to_patch(images)
        assert (patches * patches).sum() > 1e-6
        patches = self.knowledge(patches, embs, locations)
        assert (patches * patches).sum() > 1e-6
        results = self.dense(patches)
        assert (results * results).sum() > 1e-6
        inv_depth = self.head(results)
        # try:
        #     assert (inv_depth ** 2).sum() > 1e-6
        # except:
        #     print(inv_depth)
        #     assert False

        if self.invert:
            try:
                depth = self.scale * inv_depth + self.shift
            except:
                print(self.scale, self.shift)
                print(inv_depth.shape)
                assert False
                
            depth[depth < 1e-8] = 1e-8
            depth = 1.0 / depth
        else:
            depth = inv_depth

        depth = F.interpolate(
            depth,
            size=images.shape[2:4],
            mode="bilinear",
            align_corners=False
        )

        return F.softplus(depth)
